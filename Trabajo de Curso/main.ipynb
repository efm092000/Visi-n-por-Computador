{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736417497.311849  399442 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736417497.315159  400445 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.vision.gesture_recognizer import GestureRecognizerResult\n",
    "from mediapipe.tasks.python.vision.pose_landmarker import PoseLandmarkerResult\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# os is used for Ilka's linux :)\n",
    "import os\n",
    "os.environ[\"QT_QPA_PLATFORM\"] = \"xcb\"\n",
    "\n",
    "# settings\n",
    "threshold_mask = True\n",
    "fps = 10\n",
    "frame_scale = 0 # in percentage, 0 for deactivation\n",
    "clear_mem_in_process = False\n",
    "\n",
    "# Initialization of Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "model_path_gesture = './gesture_recognizer.task'\n",
    "model_path_pose = './pose_landmarker_full.task'\n",
    "\n",
    "# Configuration variables\n",
    "colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255)]  # Red, Green, Blue, Yellow\n",
    "current_color = colors[0]\n",
    "brush_size = 8\n",
    "menu_width = 100\n",
    "menu_dist_y = 0\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "drawing = False\n",
    "erasing = False\n",
    "single_finger = True\n",
    "dominant_hand = None\n",
    "calibrating = True\n",
    "\n",
    "frame_width = None\n",
    "frame_height = None\n",
    "canvas = None\n",
    "segment_mask = None\n",
    "\n",
    "current_frame = None\n",
    "gesture_frame = None\n",
    "frame_to_save = None\n",
    "layers = [\"BACK\", \"FRONT\"]\n",
    "finger_layer = None # layer for colored point when drawing or circle when erasing \n",
    "current_layer = 0\n",
    "\n",
    "last_T_pose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1736417497.429988  400438 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def draw_menu(frame):\n",
    "    global menu_dist_y\n",
    "    menu_dist_y = int(frame_height / len(colors) /2)\n",
    "    # color menu\n",
    "    for i, color in enumerate(colors):\n",
    "        cx, cy = int(menu_width/2), menu_dist_y + i * menu_dist_y*2\n",
    "        cv2.circle(frame, (cx, cy), 30, color, -1)\n",
    "\n",
    "    # layer menu\n",
    "    for j, layer in enumerate(layers):\n",
    "        cx, cy = frame_width - int(menu_width/2), menu_dist_y + j * menu_dist_y*2\n",
    "        cv2.circle(frame, (cx, cy), 30, (255, 255, 255), -1)\n",
    "        # center text\n",
    "        text_size = cv2.getTextSize(layer, FONT, 0.6, 1)[0]\n",
    "        text_x = int(cx - text_size[0] // 2) \n",
    "        text_y = int(cy + text_size[1] // 2)\n",
    "\n",
    "        cv2.putText(frame, layer, (text_x, text_y), FONT, 0.6, (0, 0, 0), 1)\n",
    "\n",
    "\n",
    "def count_visible_fingers(hand_landmarks):\n",
    "    finger_tips = [\n",
    "        mp_hands.HandLandmark.THUMB_TIP,\n",
    "        mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.PINKY_TIP\n",
    "    ]\n",
    "    visible_fingers = 0\n",
    "    for tip in finger_tips:\n",
    "        tip_coord = hand_landmarks[tip]\n",
    "        base_coord = hand_landmarks[tip - 2]  # Base of each finger\n",
    "        if tip_coord.y < base_coord.y:  # Finger is up if tip is above base\n",
    "            visible_fingers += 1\n",
    "    return visible_fingers\n",
    "\n",
    "def calculate_brush_size(hand_landmarks):\n",
    "    index_tip = hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    distance = math.sqrt((index_tip.x - middle_tip.x)**2 + (index_tip.y - middle_tip.y)**2)\n",
    "    return int(distance * 300)\n",
    "\n",
    "def menu_interaction(finger_x, finger_y):\n",
    "    global current_color, current_layer\n",
    "    # color menu\n",
    "    if finger_x < menu_width:\n",
    "        # Calculate which color zone the index finger is in\n",
    "        for i, color in enumerate(colors):\n",
    "            layer_y = menu_dist_y + i * menu_dist_y*2  # Y position of each color selector\n",
    "            # Check if index finger is within the vertical range of this color\n",
    "            if abs(finger_y - layer_y) < 30:  # 30 pixel threshold for selection\n",
    "                current_color = color\n",
    "                break\n",
    "    # layer menu\n",
    "    if finger_x > frame_width - menu_width:\n",
    "        # Calculate which zone the index finger is in\n",
    "        for i in range(len(layers)):\n",
    "            layer_y = menu_dist_y + i * menu_dist_y*2  # Y position of each selector\n",
    "            # Check if index finger is within the vertical range of this button\n",
    "            if abs(finger_y - layer_y) < 30:  # 30 pixel threshold for selection\n",
    "                current_layer = i\n",
    "                break\n",
    "\n",
    "def detect_dominant_hand(result):\n",
    "    global calibrating\n",
    "    if result.hand_landmarks and result.handedness and result.gestures:\n",
    "        for hand_idx in range(len(result.hand_landmarks)):\n",
    "            hand_label = result.handedness[hand_idx][0].category_name\n",
    "            gesture_name = result.gestures[hand_idx][0].category_name\n",
    "            if gesture_name == \"Open_Palm\":\n",
    "                calibrating = False\n",
    "                return hand_label\n",
    "    return None\n",
    "\n",
    "def process_dominant_hand_interaction(frame, hand_landmarks, gesture):\n",
    "    global drawing, erasing, single_finger, current_color, brush_size, canvas, finger_layer\n",
    "\n",
    "    # reset finger layer\n",
    "    finger_layer = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "    drawing = False\n",
    "    erasing = False\n",
    "    single_finger = False\n",
    "    if gesture == \"Open_Palm\":\n",
    "        drawing = False\n",
    "        print('open palm')\n",
    "    elif gesture == \"Pointing_Up\":\n",
    "        drawing = True\n",
    "        single_finger = True\n",
    "        brush_size = 8\n",
    "        print('pointing up')\n",
    "    elif gesture == \"Victory\":\n",
    "        drawing = True\n",
    "        brush_size = calculate_brush_size(hand_landmarks)\n",
    "        print('victory')\n",
    "    elif gesture == \"Closed_Fist\":\n",
    "        drawing = False\n",
    "        erasing = True\n",
    "\n",
    "\n",
    "    # Get index finger coordinates\n",
    "    index_finger = hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    x, y = int(index_finger.x * frame_width), int(index_finger.y * frame_height)\n",
    "\n",
    "    menu_interaction(x,y)\n",
    "\n",
    "    if not (x < menu_width and x > frame_width - menu_width): # if finger is not in menu area\n",
    "        if drawing:\n",
    "            if single_finger:\n",
    "                cv2.circle(canvas[current_layer], (x, y), brush_size, current_color, -1)\n",
    "            else:\n",
    "                middle_finger = hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                # Calculate rectangle parameters between fingers\n",
    "                x1 = int(index_finger.x * frame_width)\n",
    "                y1 = int(index_finger.y * frame_height)\n",
    "                x2 = int(middle_finger.x * frame_width)\n",
    "                y2 = int(middle_finger.y * frame_height)\n",
    "                \n",
    "                # Calculate center point of rectangle\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                \n",
    "                # Calculate rotation angle\n",
    "                angle = math.atan2(y2 - y1, x2 - x1)\n",
    "                \n",
    "                # Create rectangle points\n",
    "                rect_length = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "                rect_height = 10  # Small fixed height\n",
    "                \n",
    "                # Calculate rectangle corners\n",
    "                cos_angle = math.cos(angle)\n",
    "                sin_angle = math.sin(angle)\n",
    "                \n",
    "                # Define rectangle points\n",
    "                points = np.array([\n",
    "                    [-rect_length/2, -rect_height/2],\n",
    "                    [rect_length/2, -rect_height/2],\n",
    "                    [rect_length/2, rect_height/2],\n",
    "                    [-rect_length/2, rect_height/2]\n",
    "                ], dtype=np.float32)\n",
    "                \n",
    "                # Rotate points\n",
    "                rotated_points = np.array([\n",
    "                    [cos_angle, -sin_angle],\n",
    "                    [sin_angle, cos_angle]\n",
    "                ]) @ points.T\n",
    "                \n",
    "                # Translate points to center position\n",
    "                final_points = (rotated_points.T + [center_x, center_y]).astype(np.int32)\n",
    "                \n",
    "                # Draw the rotated rectangle\n",
    "                cv2.fillPoly(canvas[current_layer], [final_points], current_color)\n",
    "        if erasing:\n",
    "            # calculate hand middle point\n",
    "            wrist = hand_landmarks[mp_hands.HandLandmark.WRIST]\n",
    "            wrist_x, wrist_y = int(wrist.x * frame_width), int(wrist.y * frame_height)\n",
    "            middle_finger_mcp = hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "            middle_finger_mcp_x, middle_finger_mcp_y = int(middle_finger_mcp.x * frame_width), int(middle_finger_mcp.y * frame_height)\n",
    "            middle_x = int((wrist_x + middle_finger_mcp_x) /2)\n",
    "            middle_y = int((wrist_y + middle_finger_mcp_y) /2)\n",
    "            eraser_size = int(math.sqrt((wrist_x - middle_finger_mcp_x)**2 + (wrist_y - middle_finger_mcp_y)**2) /2)\n",
    "            # erase\n",
    "            cv2.circle(canvas[current_layer], (middle_x, middle_y), eraser_size, (0,0,0), -1)\n",
    "            # show circle of eraser\n",
    "            cv2.circle(finger_layer, (middle_x, middle_y), eraser_size, (255,255,255), 2)\n",
    "\n",
    "\n",
    "    # Show index finger color\n",
    "    if not erasing:\n",
    "        cv2.circle(finger_layer, (x, y), 10, current_color, -1)\n",
    "\n",
    "def callback_gesture(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global calibrating, dominant_hand, canvas, frame_width, frame_height, gesture_frame\n",
    "    \n",
    "    frame = current_frame.copy()\n",
    "\n",
    "    if canvas is None:\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        canvas = []\n",
    "        for i in range(len(layers)):\n",
    "            canvas.append(np.zeros((frame_height, frame_width, 3), dtype=np.uint8)) \n",
    "    \n",
    "    if calibrating:\n",
    "        cv2.putText(frame, \"Show your dominant hand\", (menu_width, 50), \n",
    "                    FONT, 1, (0, 255, 255), 2)\n",
    "        if result.gestures:\n",
    "            dominant_hand = detect_dominant_hand(result)\n",
    "            if dominant_hand:\n",
    "                print('dominant hand set to', dominant_hand)\n",
    "    else: # paint\n",
    "        if result.hand_landmarks and result.handedness:\n",
    "            for i, hand_landmarks in enumerate(result.hand_landmarks):\n",
    "                handedness = result.handedness[i][0].category_name\n",
    "                gesture = result.gestures[i][0].category_name\n",
    "                if handedness == dominant_hand:\n",
    "                    process_dominant_hand_interaction(frame, hand_landmarks, gesture)\n",
    "    \n",
    "    # Combine all canvas layers with frame\n",
    "    for i in range(len(layers)):\n",
    "        frame = cv2.addWeighted(frame, 1.0, canvas[i], 1.0, 0)\n",
    "    \n",
    "    gesture_frame = frame\n",
    "\n",
    "\n",
    "def callback_pose(result: PoseLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global processed_frame, canvas, last_T_pose, segment_mask\n",
    "    if not result.pose_landmarks:\n",
    "        return\n",
    "    \n",
    "    landmarks = result.pose_landmarks[0]\n",
    "    if result.segmentation_masks:\n",
    "        segment_mask = np.array(result.segmentation_masks[0].numpy_view())\n",
    "        if threshold_mask:\n",
    "            _, segment_mask = cv2.threshold(segment_mask, 0.5, 1, cv2.THRESH_BINARY)\n",
    "    else:\n",
    "        segment_mask = None\n",
    "    \n",
    "    left_shoulder = landmarks[11]  \n",
    "    right_shoulder = landmarks[12]  \n",
    "    left_wrist = landmarks[15]     \n",
    "    right_wrist = landmarks[16]    \n",
    "    \n",
    "    # Check if arms are horizontal (y-coordinates approximately equal)\n",
    "    shoulder_wrist_y_diff_left = abs(left_shoulder.y - left_wrist.y)\n",
    "    shoulder_wrist_y_diff_right = abs(right_shoulder.y - right_wrist.y)\n",
    "    \n",
    "    # Check if arms are extended (x-coordinates significantly different)\n",
    "    shoulder_wrist_x_diff_left = abs(left_shoulder.x - left_wrist.x)\n",
    "    shoulder_wrist_x_diff_right = abs(right_shoulder.x - right_wrist.x)\n",
    "    \n",
    "    Y_THRESHOLD = 0.1\n",
    "    X_THRESHOLD = 0.2\n",
    "    \n",
    "    is_t_pose = (\n",
    "        shoulder_wrist_y_diff_left < Y_THRESHOLD and\n",
    "        shoulder_wrist_y_diff_right < Y_THRESHOLD and\n",
    "        shoulder_wrist_x_diff_left > X_THRESHOLD and\n",
    "        shoulder_wrist_x_diff_right > X_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # you always have at least 5 seconds between the savings\n",
    "    if is_t_pose and (time.time() - last_T_pose) > 5 and frame_to_save is not None:\n",
    "        filename = 'drawing-' + str(int(time.time())) + '.png'\n",
    "        cv2.imwrite(filename, frame_to_save)\n",
    "        print('image saved as ' + filename)\n",
    "        last_T_pose = time.time()\n",
    "        # reset canvas\n",
    "        canvas = None\n",
    "\n",
    "# Optimizations\n",
    "# Frame Processing Rate Control\n",
    "def limit_frame_rate(cap, target_fps=30):\n",
    "    # Limit frame processing rate\n",
    "    cap.set(cv2.CAP_PROP_FPS, target_fps)\n",
    "    return 1.0 / target_fps\n",
    "\n",
    "# Frame Resizing\n",
    "def resize_frame(frame, scale_percent=50):\n",
    "    # Resize frame to reduce processing load\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    return cv2.resize(frame, (width, height))\n",
    "\n",
    "# Memory Management\n",
    "def clear_memory():\n",
    "    # Clear unused memory periodically\n",
    "    if 'cv2' in globals():\n",
    "        cv2.destroyAllWindows()\n",
    "    if 'mp' in globals():\n",
    "        mp.solutions.hands.Hands().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1736417497.507217  400437 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1736417497.513314  399442 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736417497.516680  400449 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "W0000 00:00:1736417497.518462  399442 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1736417497.522536  399442 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1736417497.629963  400455 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736417497.676697  400451 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736417497.678834  400454 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736417497.679115  400454 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1736417497.688179  399442 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736417497.690116  400461 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "W0000 00:00:1736417497.884409  400465 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736417497.938727  400462 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant hand set to Left\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "pointing up\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n",
      "victory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736417524.260879  399442 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736417524.264395  400633 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "W0000 00:00:1736417524.358784  400624 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736417524.398020  400627 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Set up gesture recognizer\n",
    "options_gesture = mp.tasks.vision.GestureRecognizerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path_gesture),\n",
    "    running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,\n",
    "    result_callback=callback_gesture\n",
    ")\n",
    "options_pose = mp.tasks.vision.PoseLandmarkerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path_pose),\n",
    "    running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,\n",
    "    result_callback=callback_pose,\n",
    "    output_segmentation_masks=True\n",
    ")\n",
    "\n",
    "recognizer_gesture = mp.tasks.vision.GestureRecognizer.create_from_options(options_gesture)\n",
    "pose_landmarker = mp.tasks.vision.PoseLandmarker.create_from_options(options_pose)\n",
    "\n",
    "# Start camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_delay = limit_frame_rate(cap, target_fps=fps)  # Reduce FPS\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Resize frame to reduce processing load\n",
    "        if frame_scale:\n",
    "            frame = resize_frame(frame, scale_percent=frame_scale)\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert only once and reuse\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        current_frame = frame.copy()\n",
    "        \n",
    "        # Use numpy view instead of copy when possible\n",
    "        mp_image = mp.Image(\n",
    "            image_format=mp.ImageFormat.SRGB, \n",
    "            data=rgb_frame\n",
    "        )\n",
    "        \n",
    "        current_timestamp_ms = int(time.time() * 1000)\n",
    "        \n",
    "        # Batch process recognitions\n",
    "        recognizer_gesture.recognize_async(mp_image, current_timestamp_ms)\n",
    "        pose_landmarker.detect_async(mp_image, current_timestamp_ms)\n",
    "        \n",
    "        if gesture_frame is not None and canvas is not None:\n",
    "            composite_frame = gesture_frame.copy()\n",
    "            # Add back layer\n",
    "            composite_frame = cv2.addWeighted(composite_frame, 1.0, canvas[0], 1.0, 0)\n",
    "            \n",
    "            # Add masked camera frame (person)\n",
    "            if segment_mask is not None:\n",
    "                mask_3channel = (segment_mask * 255).astype(np.uint8)\n",
    "                mask_3channel = cv2.cvtColor(mask_3channel, cv2.COLOR_GRAY2BGR)\n",
    "                \n",
    "                inv_mask = cv2.bitwise_not(mask_3channel)\n",
    "                masked_frame = cv2.bitwise_and(frame, mask_3channel)\n",
    "                masked_composite = cv2.bitwise_and(composite_frame, inv_mask)\n",
    "                composite_frame = cv2.add(masked_composite, masked_frame)\n",
    "            \n",
    "            # Add front layer\n",
    "            composite_frame = cv2.addWeighted(composite_frame, 1.0, canvas[1], 1.0, 0)\n",
    "            frame_to_save = composite_frame\n",
    "\n",
    "            # Add 'finger layer' \n",
    "            if finger_layer is not None:\n",
    "                composite_frame = cv2.addWeighted(composite_frame, 1.0, finger_layer, 1.0, 0)\n",
    "            \n",
    "            draw_menu(composite_frame)\n",
    "            cv2.imshow(\"Interactive Paint\", composite_frame)\n",
    "        else:\n",
    "            cv2.imshow(\"Interactive Paint\", frame)\n",
    "            \n",
    "        if cv2.waitKey(int(frame_delay * 1000)) & 0xFF == 27:\n",
    "            break\n",
    "            \n",
    "        # Clear memory periodically, every 30 seconds\n",
    "        if clear_mem_in_process and current_timestamp_ms % 30000 == 0: \n",
    "            clear_memory()\n",
    "            \n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose_landmarker.close()\n",
    "    recognizer_gesture.close()\n",
    "    clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
