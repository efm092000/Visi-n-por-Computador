{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736180102.389456   92961 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736180102.392169   93071 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.vision.gesture_recognizer import GestureRecognizerResult\n",
    "from mediapipe.tasks.python.vision.pose_landmarker import PoseLandmarkerResult\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Initialization of Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "model_path_gesture = './gesture_recognizer.task'\n",
    "model_path_pose = './pose_landmarker_full.task'\n",
    "\n",
    "# Configuration variables\n",
    "colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255)]  # Red, Green, Blue, Yellow\n",
    "current_color = colors[0]\n",
    "brush_size = 10\n",
    "menu_width = 100\n",
    "drawing = False\n",
    "erasing = False\n",
    "single_finger = True\n",
    "canvas = None\n",
    "dominant_hand = None\n",
    "calibrating = True\n",
    "frame_width = None\n",
    "frame_height = None\n",
    "current_frame = None\n",
    "processed_frame = None\n",
    "last_T_pose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1736180102.434941   93062 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1736180102.467909   93066 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def draw_menu(frame):\n",
    "    for i, color in enumerate(colors):\n",
    "        cx, cy = 50, 100 + i * 100\n",
    "        cv2.circle(frame, (cx, cy), 30, color, -1)\n",
    "\n",
    "def count_visible_fingers(hand_landmarks):\n",
    "    finger_tips = [\n",
    "        mp_hands.HandLandmark.THUMB_TIP,\n",
    "        mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "        mp_hands.HandLandmark.PINKY_TIP\n",
    "    ]\n",
    "    visible_fingers = 0\n",
    "    for tip in finger_tips:\n",
    "        tip_coord = hand_landmarks[tip]\n",
    "        base_coord = hand_landmarks[tip - 2]  # Base of each finger\n",
    "        if tip_coord.y < base_coord.y:  # Finger is up if tip is above base\n",
    "            visible_fingers += 1\n",
    "    return visible_fingers\n",
    "\n",
    "def calculate_brush_size(hand_landmarks):\n",
    "    index_tip = hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    distance = math.sqrt((index_tip.x - middle_tip.x)**2 + (index_tip.y - middle_tip.y)**2)\n",
    "    return int(distance * 300)\n",
    "\n",
    "def detect_dominant_hand(result):\n",
    "    global calibrating\n",
    "    if result.hand_landmarks and result.handedness and result.gestures:\n",
    "        for hand_idx in range(len(result.hand_landmarks)):\n",
    "            hand_label = result.handedness[hand_idx][0].category_name\n",
    "            gesture_name = result.gestures[hand_idx][0].category_name\n",
    "            if gesture_name == \"Open_Palm\":\n",
    "                calibrating = False\n",
    "                return hand_label\n",
    "    return None\n",
    "\n",
    "def process_dominant_hand_interaction(frame, hand_landmarks, gesture):\n",
    "    global drawing, erasing, single_finger, current_color, brush_size, canvas\n",
    "\n",
    "    drawing = False\n",
    "    erasing = False\n",
    "    single_finger = False\n",
    "    if gesture == \"Open_Palm\":\n",
    "        drawing = False\n",
    "        print('open palm')\n",
    "    elif gesture == \"Pointing_Up\":\n",
    "        drawing = True\n",
    "        single_finger = True\n",
    "        brush_size = 10\n",
    "        print('pointing up')\n",
    "    elif gesture == \"Victory\":\n",
    "        drawing = True\n",
    "        brush_size = calculate_brush_size(hand_landmarks)\n",
    "        print('victory')\n",
    "    elif gesture == \"Closed_Fist\":\n",
    "        drawing = False\n",
    "        erasing = True\n",
    "\n",
    "\n",
    "    # Get index finger coordinates\n",
    "    index_finger = hand_landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    x, y = int(index_finger.x * frame_width), int(index_finger.y * frame_height)\n",
    "\n",
    "    # Count visible fingers\n",
    "    visible_fingers = count_visible_fingers(hand_landmarks)\n",
    "    \n",
    "    # if visible_fingers == 0:\n",
    "    #     drawing = drawing # False\n",
    "    # else:\n",
    "    #     #drawing = True\n",
    "    #     if visible_fingers == 1:\n",
    "    #         brush_size = 10\n",
    "    #     elif visible_fingers >= 2:\n",
    "    #         brush_size = calculate_brush_size(hand_landmarks)\n",
    "\n",
    "    # Menu interaction\n",
    "    if x < menu_width:\n",
    "        # Calculate which color zone the index finger is in\n",
    "        for i, color in enumerate(colors):\n",
    "            color_y = 100 + i * 100  # Y position of each color selector\n",
    "            # Check if index finger is within the vertical range of this color\n",
    "            if abs(y - color_y) < 30:  # 30 pixel threshold for selection\n",
    "                current_color = color\n",
    "                break\n",
    "    else:\n",
    "        if drawing:\n",
    "            if single_finger:\n",
    "                cv2.circle(canvas, (x, y), brush_size, current_color, -1)\n",
    "            else:\n",
    "                middle_finger = hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                # Calculate rectangle parameters between fingers\n",
    "                x1 = int(index_finger.x * frame_width)\n",
    "                y1 = int(index_finger.y * frame_height)\n",
    "                x2 = int(middle_finger.x * frame_width)\n",
    "                y2 = int(middle_finger.y * frame_height)\n",
    "                \n",
    "                # Calculate center point of rectangle\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                \n",
    "                # Calculate rotation angle\n",
    "                angle = math.atan2(y2 - y1, x2 - x1)\n",
    "                \n",
    "                # Create rectangle points\n",
    "                rect_length = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "                rect_height = 10  # Small fixed height\n",
    "                \n",
    "                # Calculate rectangle corners\n",
    "                cos_angle = math.cos(angle)\n",
    "                sin_angle = math.sin(angle)\n",
    "                \n",
    "                # Define rectangle points\n",
    "                points = np.array([\n",
    "                    [-rect_length/2, -rect_height/2],\n",
    "                    [rect_length/2, -rect_height/2],\n",
    "                    [rect_length/2, rect_height/2],\n",
    "                    [-rect_length/2, rect_height/2]\n",
    "                ], dtype=np.float32)\n",
    "                \n",
    "                # Rotate points\n",
    "                rotated_points = np.array([\n",
    "                    [cos_angle, -sin_angle],\n",
    "                    [sin_angle, cos_angle]\n",
    "                ]) @ points.T\n",
    "                \n",
    "                # Translate points to center position\n",
    "                final_points = (rotated_points.T + [center_x, center_y]).astype(np.int32)\n",
    "                \n",
    "                # Draw the rotated rectangle\n",
    "                cv2.fillPoly(canvas, [final_points], current_color)\n",
    "        if erasing:\n",
    "            # calculate hand middle point\n",
    "            wrist = hand_landmarks[mp_hands.HandLandmark.WRIST]\n",
    "            wrist_x, wrist_y = int(wrist.x * frame_width), int(wrist.y * frame_height)\n",
    "            middle_finger_mcp = hand_landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_MCP]\n",
    "            middle_finger_mcp_x, middle_finger_mcp_y = int(middle_finger_mcp.x * frame_width), int(middle_finger_mcp.y * frame_height)\n",
    "            middle_x = int((wrist_x + middle_finger_mcp_x) /2)\n",
    "            middle_y = int((wrist_y + middle_finger_mcp_y) /2)\n",
    "            eraser_size = int(math.sqrt((wrist_x - middle_finger_mcp_x)**2 + (wrist_y - middle_finger_mcp_y)**2) /2)\n",
    "            # erase\n",
    "            cv2.circle(canvas, (middle_x, middle_y), eraser_size, (0,0,0), -1)\n",
    "            # show circle of eraser\n",
    "            cv2.circle(frame, (middle_x, middle_y), eraser_size, (0,0,0), 2)\n",
    "\n",
    "\n",
    "    # Show index finger color\n",
    "    if not erasing:\n",
    "        cv2.circle(frame, (x, y), 10, current_color, -1)\n",
    "\n",
    "def callback_gesture(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global calibrating, dominant_hand, canvas, frame_width, frame_height, current_frame, processed_frame\n",
    "    \n",
    "    frame = current_frame.copy()\n",
    "\n",
    "    if canvas is None:\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    if calibrating:\n",
    "        cv2.putText(frame, \"Show your dominant hand\", (10, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        if result.gestures:\n",
    "            dominant_hand = detect_dominant_hand(result)\n",
    "            if dominant_hand:\n",
    "                print('dominant hand set to', dominant_hand)\n",
    "    else: # paint\n",
    "        if result.hand_landmarks and result.handedness:\n",
    "            for i, hand_landmarks in enumerate(result.hand_landmarks):\n",
    "                handedness = result.handedness[i][0].category_name\n",
    "                gesture = result.gestures[i][0].category_name\n",
    "                if handedness == dominant_hand:\n",
    "                    process_dominant_hand_interaction(frame, hand_landmarks, gesture)\n",
    "    \n",
    "    # Combine canvas with frame\n",
    "    frame = cv2.addWeighted(frame, 0.5, canvas, 0.5, 0)\n",
    "    draw_menu(frame)\n",
    "    \n",
    "    processed_frame = frame\n",
    "\n",
    "\n",
    "# print('pose landmarker result: {}'.format(result))\n",
    "def callback_pose(result: PoseLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global processed_frame, canvas, last_T_pose\n",
    "    if not result.pose_landmarks:\n",
    "        return\n",
    "    \n",
    "    landmarks = result.pose_landmarks[0]\n",
    "    \n",
    "    # Get shoulder and wrist coordinates\n",
    "    left_shoulder = landmarks[11]  # LEFT_SHOULDER\n",
    "    right_shoulder = landmarks[12]  # RIGHT_SHOULDER\n",
    "    left_wrist = landmarks[15]     # LEFT_WRIST\n",
    "    right_wrist = landmarks[16]    # RIGHT_WRIST\n",
    "    \n",
    "    # Check if arms are horizontal (y-coordinates approximately equal)\n",
    "    shoulder_wrist_y_diff_left = abs(left_shoulder.y - left_wrist.y)\n",
    "    shoulder_wrist_y_diff_right = abs(right_shoulder.y - right_wrist.y)\n",
    "    \n",
    "    # Check if arms are extended (x-coordinates significantly different)\n",
    "    shoulder_wrist_x_diff_left = abs(left_shoulder.x - left_wrist.x)\n",
    "    shoulder_wrist_x_diff_right = abs(right_shoulder.x - right_wrist.x)\n",
    "    \n",
    "    # Thresholds for T-pose detection\n",
    "    Y_THRESHOLD = 0.1\n",
    "    X_THRESHOLD = 0.2\n",
    "    \n",
    "    is_t_pose = (\n",
    "        shoulder_wrist_y_diff_left < Y_THRESHOLD and\n",
    "        shoulder_wrist_y_diff_right < Y_THRESHOLD and\n",
    "        shoulder_wrist_x_diff_left > X_THRESHOLD and\n",
    "        shoulder_wrist_x_diff_right > X_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # you always have at least 5 seconds between the savings\n",
    "    if is_t_pose and (time.time() - last_T_pose) > 5:\n",
    "        filename = 'drawing-' + str(time.time()) + '.png'\n",
    "        print('saving image as ' + filename)\n",
    "        cv2.imwrite(filename, processed_frame)\n",
    "        last_T_pose = time.time()\n",
    "        # reset canvas\n",
    "        canvas = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736180107.270077   92961 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736180107.271925   93171 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "W0000 00:00:1736180107.272635   92961 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "I0000 00:00:1736180107.274390   92961 hand_gesture_recognizer_graph.cc:250] Custom gesture classifier is not defined.\n",
      "W0000 00:00:1736180107.331125   93172 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736180107.366250   93173 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736180107.367904   93174 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736180107.368094   93174 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1736180107.373540   92961 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1736180107.375821   93183 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.9-0ubuntu0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "W0000 00:00:1736180107.491172   93191 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736180107.545123   93189 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736180108.079133   93172 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/jeski/sem/vc/Visi-n-por-Computador/.venvt/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominant hand set to Left\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n",
      "open palm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set up gesture recognizer\n",
    "options_gesture = mp.tasks.vision.GestureRecognizerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path_gesture),\n",
    "    running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,\n",
    "    result_callback=callback_gesture\n",
    ")\n",
    "options_pose = mp.tasks.vision.PoseLandmarkerOptions(\n",
    "    base_options=mp.tasks.BaseOptions(model_asset_path=model_path_pose),\n",
    "    running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,\n",
    "    result_callback=callback_pose)\n",
    "\n",
    "recognizer_gesture = mp.tasks.vision.GestureRecognizer.create_from_options(options_gesture)\n",
    "pose_landmarker = mp.tasks.vision.PoseLandmarker.create_from_options(options_pose)\n",
    "\n",
    "# Start camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)  # Horizontal flip for mirror effect\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Store the current frame for the callback to use\n",
    "    current_frame = frame.copy()\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "    current_timestamp_ms = int(time.time() * 1000)\n",
    "\n",
    "    # pose and gesture recognition\n",
    "    recognizer_gesture.recognize_async(mp_image, current_timestamp_ms)\n",
    "    pose_landmarker.detect_async(mp_image, current_timestamp_ms)\n",
    "\n",
    "    # Display the processed frame if available\n",
    "    if processed_frame is not None:\n",
    "        cv2.imshow(\"Interactive Paint\", processed_frame)\n",
    "    else:\n",
    "        cv2.imshow(\"Interactive Paint\", frame)\n",
    "    \n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "    elif key & 0xFF == ord('d'):  # Toggle drawing\n",
    "        drawing = not drawing\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
