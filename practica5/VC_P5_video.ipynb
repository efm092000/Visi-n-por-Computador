{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trap_order = [\n",
    "    0, 0, 1, 0, 1, 0, 0\n",
    "]\n",
    "emotion_order = [\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"neutral\",\n",
    "    \"neutral\",\n",
    "    \"angry\",\n",
    "    \"happy\",\n",
    "    \"happy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "# Configuración de los modelos de detección de caras\n",
    "haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "dnn_model = \"deploy.prototxt.txt\"\n",
    "dnn_weights = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(dnn_model, dnn_weights)\n",
    "mtcnn_det = MTCNN()\n",
    "\n",
    "# Cargar emojis para cada emoción\n",
    "emojisDict = {\n",
    "    \"happy\": cv2.imread(\"images/happy.png\", cv2.IMREAD_UNCHANGED),\n",
    "    \"sad\": cv2.imread(\"images/sad.png\", cv2.IMREAD_UNCHANGED),\n",
    "    \"fear\": cv2.imread(\"images/fear.png\", cv2.IMREAD_UNCHANGED),\n",
    "    \"neutral\": cv2.imread(\"images/neutral.png\", cv2.IMREAD_UNCHANGED),\n",
    "    \"disgust\": cv2.imread(\"images/disgust.png\", cv2.IMREAD_UNCHANGED),\n",
    "    \"angry\": cv2.imread(\"images/angry.png\", cv2.IMREAD_UNCHANGED),\n",
    "    \"surprise\": cv2.imread(\"images/surprised.png\", cv2.IMREAD_UNCHANGED)\n",
    "}\n",
    "\n",
    "# Variables del juego\n",
    "sequence_index = 0\n",
    "score = 0\n",
    "countdown_time = 8\n",
    "time_to_react = 2  # You have two seconds to change your emotion before it is counted\n",
    "reset_time = True\n",
    "remaining_time = countdown_time\n",
    "start_time = time.time()\n",
    "det = 0\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Bucle principal del juego\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Calcular el tiempo transcurrido para la cuenta regresiva\n",
    "    elapsed_time = time.time() - start_time\n",
    "    remaining_time = countdown_time - int(elapsed_time)\n",
    "\n",
    "    # Reiniciar el juego después de cada cuenta regresiva\n",
    "    if reset_time or remaining_time <= 0:\n",
    "        remaining_time = countdown_time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Obtener el siguiente estado de la secuencia\n",
    "        if sequence_index < len(trap_order):\n",
    "            is_trap = trap_order[sequence_index]\n",
    "            current_emotion = emotion_order[sequence_index]\n",
    "            instruction_text = f\"Be {current_emotion}\" if is_trap else f\"Simon says: Be {current_emotion}\"\n",
    "            sequence_index += 1\n",
    "        else:\n",
    "            break  # End game if sequence is exhausted\n",
    "        reset_time = False\n",
    "\n",
    "    # Cambiar de detector de caras según el valor de `det`\n",
    "    faces = []\n",
    "    if det == 0:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    elif det == 1:\n",
    "        h, w = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104.0, 177.0, 123.0], False, False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (x, y, x2, y2) = box.astype(\"int\")\n",
    "                faces.append((x, y, x2 - x, y2 - y))\n",
    "    else:  # case det == 2\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mtcnn_det.detect_faces(rgb_frame)\n",
    "        faces = [(result['box'][0], result['box'][1], result['box'][2], result['box'][3]) for result in results]\n",
    "\n",
    "    if remaining_time <= (countdown_time - time_to_react):\n",
    "        # Detectar emociones y superponer el emoji correspondiente para cada cara\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            try:\n",
    "                # Analizar la emoción de la cara usando DeepFace\n",
    "                analysis = DeepFace.analyze(img_path=face_img, enforce_detection=False, actions=['emotion'])\n",
    "                detected_emotion = analysis[0]['dominant_emotion']\n",
    "                \n",
    "                # Superponer el emoji de la emoción detectada\n",
    "                if detected_emotion in emojisDict:\n",
    "                    emoji = emojisDict[detected_emotion]\n",
    "                    if emoji is not None:\n",
    "                        emoji_resized = cv2.resize(emoji, (w, h))\n",
    "                        for i in range(h):\n",
    "                            for j in range(w):\n",
    "                                if emoji_resized[i, j][3] != 0:\n",
    "                                    frame[y + i, x + j] = emoji_resized[i, j][:3]\n",
    "\n",
    "                # Comprobar si la emoción detectada coincide con la instrucción actual\n",
    "                if detected_emotion == current_emotion:\n",
    "                    if not is_trap:\n",
    "                        score += 1  # Correct response\n",
    "                    else:\n",
    "                        score -= 1  # Trap triggered, wrong response\n",
    "                    reset_time = True\n",
    "                    remaining_time = countdown_time  # Reset countdown\n",
    "                    start_time = time.time()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Emotion detection error for face at {(x, y, w, h)}: {e}\")\n",
    "\n",
    "    # Mostrar la instrucción actual y otros datos en pantalla\n",
    "    cv2.rectangle(frame, (0, frame.shape[0] - 50), (frame.shape[1], frame.shape[0]), (255, 255, 255), -1)\n",
    "    cv2.putText(frame, instruction_text, (10, frame.shape[0] - 15), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.circle(frame, (frame.shape[1] - 60, 60), 30, (169, 169, 169), -1)\n",
    "    cv2.putText(frame, str(remaining_time), (frame.shape[1] - 75, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Score: {score}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Mostrar el resultado en pantalla\n",
    "    cv2.imshow('Simon Says Emotion Game', frame)\n",
    "\n",
    "    # Cambiar de detector con 'd', salir con 'Esc'\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('d'):\n",
    "        det = (det + 1) % 3\n",
    "    elif key == 27:\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar ventanas\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
